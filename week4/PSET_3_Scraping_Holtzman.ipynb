{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data & Society Problem Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Set 3 - Extend What You Have Learned\n",
    "\n",
    "Now that you know how to scrape data from Twitter, let's extend the exercise a little so you can show us what you know. This time you will set up the scraper to get data around MIT and scrape data for 20 minutes. Then you will visualize it with and visualize. Think about what you would need to change to do that.\n",
    "\n",
    "Once you have the new JSON file of Boston tweets you should a pie chart and scatterplot of your collected tweets. When you are creating your dataset, you should get at least two different attributes returned by the Twitter API (we got many of them above, so base it off of that example). Atleast one of them should be the tweet id. Make sure you remove and duplicate tweets (if any). Expanding on the above, then save the data to a CSV.\n",
    "\n",
    "Make sure you get your own Twitter Key.\n",
    "\n",
    "#### Deliverables\n",
    "***1*** - Using the Twitter REST API, collect Tweets from Boston for 30 min. Note how you set the time in the above example (in the ***run_all()*** function), it was in seconds. How would you do that here?\n",
    "\n",
    "***2*** - Create a Pie Chart showing a summary of tweets by user location. Please clean up the data so that multiple variations of the same location name are replaced with one variation.\n",
    "\n",
    "***3*** - Create a Scatterplot showing all of the tweets that had a latitude and longitude.\n",
    "\n",
    "***4*** - Pick a search term, such as Trump or #Trump and collect 15 minutes of tweets on it. Use the same lat/lon for Boston as you used above.\n",
    "\n",
    "***5*** - Export the entirety of your scraped Twitter datasets (one with a search term, one without) to two CSV files. We will be checking this CSV file for duplicates. So clean your file.\n",
    "\n",
    "#### What to Give Us on Stellar\n",
    "***1*** - Create a new Jupyter notebook that contains your scraper code. You can copy much of this one, but customize it. Submit the new Jupyter notebook, which includes your pie chart and scatterplot.\n",
    "\n",
    "***2*** - Your final CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from twython import Twython\n",
    "\n",
    "# import my twitter keys from the python file\n",
    "from twitter_key_LOCAL import api_key, api_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign the keys to the variables\n",
    "APP_KEY = api_key\n",
    "APP_SECRET = api_secret\n",
    "\n",
    "# Create a Twython object called Twitter\n",
    "# Set this up using your Twitter Keys\n",
    "# Say we are going to use OAuth 2\n",
    "twython_setup = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "\n",
    "# Get an OAuth2 access token, save as variable so we can launch our \n",
    "OAUTH2_ACCESS_TOKEN = twython_setup.obtain_access_token()\n",
    "\n",
    "# Create a Twython Object we will use for our access to the API\n",
    "my_twython = Twython(APP_KEY, access_token=OAUTH2_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QUERY TWITTER TO GET TWEETS AT MIT\n",
    "\n",
    "# Input the search term you want to search on\n",
    "search_term='beyonce' # SET A SEARCH TERM LIKE 'TRUMP'\n",
    "# CAN LEAVE search_term BLANK IF YOU WANT ALL TWEETS NEAR A SPECIFIC LOCATION\n",
    "# Setup a Lat Lon\n",
    "latlong=[40.261455,-76.882553] # Downtown Harrisburg, PA - Capital of Pennsylvania\n",
    "# Setup a search distance\n",
    "distance='25mi'\n",
    "# Set result type (can be 'recent', 'popular', or 'mixed')\n",
    "type_of_result='recent'\n",
    "# Set number of results (up to 100, remember you can only get 450 in 15 minutes)\n",
    "number_of_tweets=15\n",
    "\n",
    "\n",
    "# Fetches tweets with a given query at a given lat-long.\n",
    "def get_tweets_by_location( latlong=None ):\n",
    "    # Uses the search function to hit the APIs endpoints and look for recent tweets within the area\n",
    "    results = my_twython.search(q=search_term, geocode=str(latlong[0])+','+str(latlong[1])+','+ distance, result_type=type_of_result, count=number_of_tweets)\n",
    "    # Returns the only the statuses from the resulting JSON\n",
    "    return results['statuses']\n",
    "\n",
    "# test run our function\n",
    "get_tweets_by_location(latlong)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
